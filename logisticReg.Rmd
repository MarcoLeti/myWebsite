---
title: "Logistic Regression"
---

```{r}
library(ggplot2)
```

logistic regression hypothesis is defined as:

$h_\theta(x) = g(\theta^Tx)$

where function g is the sigmoid function. The sigmoid function is defined as:

$g(z) = \frac{1}{1+e^{-z}} = \frac{e^z}{1 + e^z}$

```{r sigmoid}
sigmoid <- function(z) {
    g <- 1 / (1 + exp(-z))
    return(g)
}

```

Logistic regression cost function:

$J(\theta) = \frac{1}{m} \displaystyle\sum_{i=1}^{m}[-y^{(i)} \log(h_\theta(x^{(i)})) - (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))]$

```{r costFunction}
costFunction <- function(theta, X, y) {
    m <- length(y)
    J <- 0
    
    J <- 1/m * ((t(log(sigmoid(X %*% theta))) %*% (-y)) 
            - t(log(1 - sigmoid(X %*% theta))) %*% (1 - y))
    
    return(J)
}
```

```{r gradient}
gradient <- function(theta, X, y) {
    m <- length(y)
    grad <- rep(0, length(theta))
    
    grad <- 1/m * t(X) %*% (sigmoid(X %*% theta) - y)
    return(grad)
}
```

```{r}
data <- read.csv("C:\\Projects\\StanfordMachineLearning\\machine-learning-ex2\\ex2\\ex2data1.txt")

colnames(data) <- c("Score1", "Score2", "Admission")
X <- as.matrix(data)
X <- cbind(matrix(rep(1, nrow(X))), X)
y <- X[, 4]
X <- X[, 1:3]
ggplot(data, aes(x = Score1, y = Score2, color = as.factor(Admission), shape = as.factor(Admission))) + geom_point(size = 2)
```

```{r}
initial_theta <- matrix(c(0, 0, 0))
theta_optim <- optim(par = initial_theta, fn = costFunction, gr = gradient, X = X, y = y, method = "BFGS")
```

This is the decision boundary of logistic regression that corresponds to the probability of 0.5
```{r decisionBoundary}
ggplot(data, aes(x = Score1, y = Score2, color = as.factor(Admission), shape = as.factor(Admission))) + geom_point(size = 2) + geom_abline(slope = (-theta_optim$par[2] / theta_optim$par[3]), intercept = (-theta_optim$par[1] / theta_optim$par[3]), colour = "blue", size = 1)

# theta_optim$par
# https://stats.stackexchange.com/questions/6206/how-to-plot-decision-boundary-in-r-for-logistic-regression-model
# http://rstudio-pubs-static.s3.amazonaws.com/320280_2a9e44636b3a4f75a7dc02bf0b167572.html
```

Prediction:
For a student with scores 45 and 85, we predict an admission
```{r prediction}
sigmoid(c(1, 45, 85) %*% theta_optim$par)
```

```{r accuracy}
prediction <- sigmoid(X %*% theta_optim$par) >= 0.5
mean(as.integer(prediction) == y) * 100
```

## Regularization

```{r costFunction}
costFunction <- function(theta, X, y, lambda) {
    m <- length(y)
    J <- 0
    
    J <- 1/m * ((t(log(sigmoid(X %*% theta))) %*% (-y)) 
            - t(log(1 - sigmoid(X %*% theta))) %*% (1 - y))
            + lambda / (2 * m) * 
    
    return(J)
}
```