sigmoid <- function(z) {
g <- 1 / (1 + exp(-z))
return(g)
}
costFunction <- function(theta, X, y) {
m <- length(y)
J <- 0
grad <- rep(0, length(theta))
J <- 1/m * ((t(log(sigmoid(X %*% theta))) %*% (-y))
- t(log(1 - sigmoid(X %*% theta))) %*% (1 - y))
return(J)
}
gradient <- function(theta, X, y) {
grad <- 1/m * t(X) %*% (sigmoid(X %*% theta) - y)
return(grad)
}
data <- read.csv("C:\\Projects\\StanfordMachineLearning\\machine-learning-ex2\\ex2\\ex2data1.txt")
colnames(data) <- c("Score1", "Score2", "Admission")
X <- as.matrix(data)
X <- cbind(matrix(rep(1, nrow(X))), X)
y <- X[, 4]
X <- X[, 1:3]
ggplot(data, aes(x = Score1, y = Score2, color = as.factor(Admission), shape = as.factor(Admission))) + geom_point(size = 2)
initial_theta <- matrix(c(0, 0, 0))
theta_optim <- optim(par = initial_theta, fn = costFunction, gr = gradient, X = X, y = y, method = "BFGS")
gradient <- function(theta, X, y) {
m <- length(y)
grad <- rep(0, length(theta))
grad <- 1/m * t(X) %*% (sigmoid(X %*% theta) - y)
return(grad)
}
initial_theta <- matrix(c(0, 0, 0))
theta_optim <- optim(par = initial_theta, fn = costFunction, gr = gradient, X = X, y = y, method = "BFGS")
View(theta_optim)
min(X$Score1)
head(X)
min(X[, 2])
min(X[, 2])-2
plot_X <- c(min(X[, 2]) - 2, max(X[, 3]) + 2)
theta_optim$par[3]
plot_X <- c(min(X[, 2]) - 2, max(X[, 3]) + 2)
plot_y <- (-1 / theta_optim$par[3]) * (theta_optim$par[2] * plot_x + theta_optim$par[1])
plot_y <- (-1 / theta_optim$par[3]) * (theta_optim$par[2] * plot_x + theta_optim$par[1])
plot_x <- c(min(X[, 2]) - 2, max(X[, 3]) + 2)
plot_y <- (-1 / theta_optim$par[3]) * (theta_optim$par[2] * plot_x + theta_optim$par[1])
theta_optim$par
rm(plot_X)
-1 / theta_optim$par[3])
(-1 / theta_optim$par[3])
theta_optim$par[2] * plot_x
(theta_optim$par[2] * plot_x + theta_optim$par[1])
plot(plot_x, plot_y)
(-1 / theta_optim$par[3]) * (theta_optim$par[2] * plot_x + theta_optim$par[1])
-24+0.19*28+0.19*100
